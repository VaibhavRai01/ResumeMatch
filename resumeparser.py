# -*- coding: utf-8 -*-
"""ResumeParser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tZRxKs2zHTwGrGdt2kiR-lLrTpxxau2M
"""

import json
import random
import logging
import spacy
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support
from spacy.training import Example
from spacy.scorer import Scorer
from sklearn.metrics import accuracy_score

def convert_dataturks_to_spacy(dataturks_JSON_FilePath):
    try:
        training_data = []
        lines=[]
        with open(dataturks_JSON_FilePath, 'r') as f:
            lines = f.readlines()

        for line in lines:
            data = json.loads(line)
            text = data['content']
            entities = []
            for annotation in data['annotation']:
                #only a single point in text annotation.
                point = annotation['points'][0]
                labels = annotation['label']
                # handle both list of labels or a single label.
                if not isinstance(labels, list):
                    labels = [labels]

                for label in labels:
                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)
                    entities.append((point['start'], point['end'] + 1 ,label))


            training_data.append((text, {"entities" : entities}))

        return training_data
    except Exception as e:
        logging.exception("Unable to process " + dataturks_JSON_FilePath + "\n" + "error = " + str(e))
        return None

def clean_annotations(train_data):
    cleaned_data = []
    for text, annotation in train_data:
        entities = annotation['entities']
        cleaned_entities = []
        last_end = -1

        for start, end, label in sorted(entities, key=lambda x: x[0]):
            if start >= last_end:  # Ensure there is no overlap
                cleaned_entities.append((start, end, label))
                last_end = end
            else:
                # Optionally, handle overlaps by merging or adjusting entities
                print(f"Found overlapping entity: {(start, end, label)} in text: '{text}'")
                # Merge entities if desired
                # Example: cleaned_entities[-1] = (cleaned_entities[-1][0], max(cleaned_entities[-1][1], end), label)

        cleaned_data.append((text, {"entities": cleaned_entities}))
    return cleaned_data

def train_spacy():

    TRAIN_DATA = convert_dataturks_to_spacy("traindata.json")
    TRAIN_DATA = clean_annotations(TRAIN_DATA)
    nlp = spacy.blank('en')  # create blank Language class
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe("ner", last=True)


    # add labels
    for _, annotations in TRAIN_DATA:
         for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(1):
            print("Statring iteration " + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                doc = nlp.make_doc(text)
                example = Example.from_dict(doc, annotations)
                try:
                  nlp.update(
                      [example],  # batch of texts
                      drop=0.2,  # dropout - make it harder to memorise data
                      sgd=optimizer,  # callable to update weights
                      losses=losses)
                except Exception as error:
                    print(error)
                    continue
            print(losses)
    #test the model and evaluate it
    TEST_DATA = convert_dataturks_to_spacy("testdata.json")
    c=0
    for text, annot in TEST_DATA:
        with open("resume" + str(c) + ".txt", "w") as f:
            doc_to_test = nlp(text)
            d = {}
            for ent in doc_to_test.ents:
                d[ent.label_] = []
            for ent in doc_to_test.ents:
                d[ent.label_].append(ent.text)

            y_true = []
            y_pred = []
            for ent in doc_to_test.ents:
              try:
                doc_gold_text = nlp.make_doc(text)
                gold = Example.from_dict(doc_gold_text, annot)
                y_true.extend([x.ent_type_ if x.ent_type_ else "Not " + ent.label_ for x in gold.reference])
                y_pred.extend([x.ent_type_ if x.ent_type_ else "Not " + ent.label_ for x in doc_to_test])
              except Exception as e:
                pass
            try:
                print("\nFor Entity " + ent.label_ + ":\n")
                print("Classification Report:\n", classification_report(y_true, y_pred))
                p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
                a = accuracy_score(y_true, y_pred)
                print("Accuracy:", a)
                print("Precision:", p)
                print("Recall:", r)
                print("F-score:", f)
            except Exception as e:
                print(e)
                pass

            c += 1
    print("Accuracy:", a)
    print("Precision:", p)
    print("Recall:", r)
    print("F-score:", f)

train_spacy()

"""Accuracy: 0.9593826157595451
Precision: 0.9351834512981235
Recall: 0.9593826157595451
F-score: 0.9456206323750364

Accuracy: 0.9634443541835905
Precision: 0.9329879420039412
Recall: 0.9634443541835905
F-score: 0.9463293162034578

Accuracy: 0.9601949634443542
Precision: 0.9336098866708394
Recall: 0.9601949634443542
F-score: 0.9457312532094156

Accuracy: 0.9634443541835905
Precision: 0.9289828043142482
Recall: 0.9634443541835905
F-score: 0.9458989924866265
"""